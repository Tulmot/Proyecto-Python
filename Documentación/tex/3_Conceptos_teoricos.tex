\capitulo{3}{Conceptos teóricos}

Este apartado va a explicar los conceptos teóricos necesarios para poder entender el proyecto.

\section{Minería de Datos}
Es conocida la frase <<los datos en bruto raramente son beneficiosos directamente>>. Puede tener valor, ya que podemos extraer información útil para la toma de decisiones o exploración, y también para la compresión del fenómeno dominante en el conjunto de datos\quote{mineria}.

La finalidad de esto es descubrir unos patrones, una similitud o una propensión que expliquen el comportamiento de los datos. Para hacer esto utiliza los métodos de la inteligencia artificial y estadística.
El objetivo del proceso de minería de datos consiste en extraer información de un conjunto de datos, luego se interpreta esta información para un uso posterior~\cite{wiki:datamining}.

La minería de datos está basada en el aprendizaje automático, para ello se considera un conjunto con $n$ muestras y se intenta predecir las propiedades de los datos desconocidos. Podemos separar los problemas de aprendizaje principalmente en dos~\cite{scikitlearn2}:
	\begin{itemize}
		\item Aprendizaje supervisado: Consiste en que a partir de un conjunto de datos, hacer predicciones basadas en el comportamiento o las características de dichos datos. Nos permite buscar patrones en datos históricos. Dos de las tareas más comunes del aprendizaje supervisado son la clasificación y la regresión:
		
		\begin{itemize}
		 	\item Clasificación: El programa debe aprender a predecir en que categoría o clase irán los nuevos datos, según las nuevas observaciones, por ejemplo, predecir si el precio de una acción bajará o subirá.
		 	\item Regresión: El programa debe predecir el valor de una variable de respuesta continua, por ejemplo, predecir las ventas de un nuevo producto.
		\end{itemize}
		
		\item Aprendizaje no supervisado: Usa datos históricos que no están etiquetados. El objetivo es explorarlos para encontrar alguna forma de organizarlos.
	\end{itemize}

La minería de datos es conocida como sinónimo de KDD (Knowledge Discovery in Databases, Descubrimiento del conocimiento en bases de datos) Este proceso tiene una secuencia de pasos~\cite{mineria_KDD}:

\begin{enumerate}[1.]
	\item Limpieza de datos. Eliminación del ruido y la inconsistencia de los datos.
	\item Integración de los datos. Múltiples fuentes de datos son combinadas.
	\item Selección de datos. Los datos relevantes para la tarea de análisis se recuperan de la base de datos.
	\item Transformación de datos. Los datos son transformados y se consolidad como apropiados para la minería mediante la realización de operaciones simétricas o de agregación.
	\item Minería de datos. Es un proceso esencial donde los métodos de inteligencia son aplicados a la extracción de patrones.
	\item Evaluación de patrones. Identificar los patrones que de verdad son interesantes, que representan el conocimiento basado en medidas de interés.
	\item Presentación del conocimiento. La visualización y el conocimiento representan técnicas que son utilizadas para presentar la minería del conocimiento a los usuarios.
\end{enumerate}

Aunque el proceso de la minería de datos consta de más etapas nosotros nos centraremos en 4 etapas~\cite{datamining}:
\begin{itemize}
	\item Determinación de los objetivos: Se tratan los objetivos que quiere conseguir el cliente bajo un asesor especialista en minería de datos.
	\item Preprocesamiento de los datos: Es la etapa que más tiempo se tarda en realizar el proceso. Se seleccionan, limpian, enriquecen, reducen y transforman las bases de datos.
	\item Determinación del modelo: Se lleva a cabo un estudio estadístico de los datos, más tarde se hace una visualización gráfica para una primera aproximación. Según los objetivos que se habían propuesto se pueden usar diferentes algoritmos de la Inteligencia Artificial.
	\item  Análisis de los resultados: Se comprueban si los datos obtenidos tienen coherencia, después se comparan con los obtenidos en los estudios estadísticos y la visualización gráfica. El cliente es el que ve si los datos le aportan nuevo conocimiento que le permita considerar sus decisiones.
\end{itemize}

\section{Multi-Label}
Una investigación en aprendizaje supervisado trata con el análisis de datos de Single-Label donde los ejemplos de entrenamiento son asociados con un Single-Label de un conjunto de disjuntos labels. Sin embargo, los ejemplos de entrenamiento en varios dominios de un aplicación, son asociado un conjunto de labels, a los que llamaremos Multi-Label~\cite{multilabel2}.

La clasificación Multi-Label es una técnica de minería de datos, la cual nos permite que de un conjunto de instancias de entrenamiento, podamos determinar a partir de unos atributos esenciales de dichas instancias para crear unas reglas que posteriormente se usarán para clasificar nuevas instancias~\cite{multilabel}. Como por ejemplo en el etiquetado de imágenes: en el que una imagen puede tener a la vez las etiquetas <<árbol>>, <<montaña>> y <<mar>>.   

\section{Ensemble}
Un ensemble es un esquema de combinación de predicciones individuales llamados clasificadores base. El éxito de un ensemble requiere tanto exactitud como diversidad de sus clasificadores base. La diversidad representa como diferente son las predicciones de los clasificadores base. Si los clasificadores base siempre están de acuerdo podría no haber diferencia entre usar sólo un clasificador base o varios combinado por un método de ensemble. Entonces el poder de usar un conjunto de clasificadores base consiste en la posibilidad que algunos de ellos pueden corregir una predicción incorrecta de otros.

Es normal obtener estos clasificadores base en un ensemble usando el mismo algoritmo, así que en esta situación el proceso de entrenamiento realizado por el ensemble es la principal fuente de diversidad. La diversidad de Bagging proviene de elegir al azar diferentes instancias para entrenar a cada clasificador de base. El método Random Subspaces elige diferentes subconjuntos de atributos para entrenar a cada clasificador base. Boosting entrena de forma iterativa el conjunto de clasificadores base, modificando los pesos de las instancias para entrenar al clasificador actual. Estos nuevos pesos se calculan a partir del error de entrenamiento en el clasificador base anterior, por lo que cada nuevo clasificador de base llega más especializado en instancias que han sido mal clasificados antes. A veces los clasificadores base son muy estables y el  algoritmo de entrenamiento del ensemble no es suficiente para proporcionar el nivel deseado de diversidad~\cite{disturbingneighbors}.

Los métodos de ensembles combinan las predicciones de unos estimadores base, que están construidos mediante un algoritmo de aprendizaje para mejorar la solidez de un solo estimador~\cite{ensemble}.
Se distinguen dos clases de métodos de ensembles:
	\begin{itemize}
		\item En los métodos de Bagging~\cite{bagging}, se construyen varios estimadores de forma independiente y luego se calcula su promedio para las predicciones.
		\item En los métodos de Boosting~\cite{boosting}, se construyen los estimadores secuencialmente y se trata de reducir el sesgo del estimador combinado. 
	\end{itemize}

\section{Disturbing Neighbors}

Los Disturbing Neighbors (DN) o vecinos molestones se han utilizado con éxito para mejorar la diversidad en los bosques~\cite{disturbingneighbors}.

El método DN trabaja en cada clasificador base de la siguiente manera:
\begin{itemize}
    \item m instancias son seleccionadas aleatoriamente de el conjunto de datos de entrenamiento para construir un clasificador 1-NN. El valor m usa valores muy pequeños.
    \item Dimensiones usadas para calcular distancia euclidea en el clasificador 1-NN son también seleccionadas aleatoriamente. Al menos el 50\% de los atributos son seleccionados.
    \item Luego m+1 nuevas características son añadidas al conjunto de entrenamiento. Una de las características adicionales es la clase predecida por el clasificador 1-NN para cada instancia x, y la otra m son características booleanas, todos los conjuntos falsos excepto uno corresponden al vecino más cercano para esa instancia.
    \item El clasificador base está entrenado usando las características originales mas las nuevas características de m+1. 
\end{itemize}
Por lo tanto, el proceso normal de entrenamiento de los clasificadores básicos se altera añadiendo estas nuevas características del clasificador 1-NN. Es por eso que el método se llama vecinos molestones. La aleatoriedad aumenta la diversidad y se debe a:
\begin{itemize}
\item Los vecinos utilizados en cada clasificador 1-NN se seleccionan aleatoriamente. Por lo tanto, sus predicciones y las características booleanas son diferentes para cada clasificador base.
\item Las dimensiones utilizadas para calcular las distancias euclidianas también se eligen de forma aleatoria, así que si dos clasificadores básicos tienen al menos los mismos m vecinos, las predicciones 1-NN y las características booleanas podrían ser diferentes.
\end{itemize}

Disturbing Neighbors es un método para alterar el proceso de entrenamiento normal de los clasificadores base en un ensemble, mejorando su diversidad y mejorando la precisión general del ensemble.

\section{Random Oracles}

Random Oracles son mini-ensembles formados por dos modelos, pueden usarse como modelos base para otros métodos ensemble. El objetivo de usar Random Oracles es tener más diversidad entre los modelos base que forman un ensemble~\cite{randomoracles}.

Dado un método base, el entrenamiento de un Random Oracle consiste en:
\begin{itemize}

	\item Seleccionar aleatoriamente un Random Oracle.
	\item Dividir los datos de entrenamiento en dos subconjuntos usando el Random Oracle.
	\item Para cada subconjunto de datos de entrenamiento, se construye un modelo. El modelo Random Oracle está formado por un par de modelos  y el propio oráculo.
\end{itemize}
La predicción del test de una instancia se realiza de la siguiente manera: 
\begin{itemize}
	\item Usa el Random Oracle para seleccionar uno de los dos modelos.
	\item Devuelve la predicción obtenida por el modelo seleccionado.
\end{itemize}

Se pueden considerar diferentes tipos de Oracles. En este trabajo, se utiliza el Linear Random Oracle. Este oráculo divide el espacio en dos subespacios utilizando un hiperplano. Para construir el oráculo, dos objetos de entrenamiento diferentes se seleccionan aleatoriamente, cada objeto de entrenamiento restante se asigna al subespacio del objeto de entrenamiento seleccionado que está más cerca.

\section{Rotation Forest}

Rotation Forests, tiene como objetivo construir clasificadores precisos y diversos. La heurística principal consiste en aplicar la extracción de características a subconjuntos de características y reconstruir un conjunto completo de características para cada clasificador en el conjunto. Elegimos el Análisis de Componentes Principales (PCA)~\cite{rotationforest}.

Para construir el clasificador se siguen los siguientes pasos:
\begin{itemize}
	\item Dividir el conjunto de datos en $K$ subconjuntos. Para maximizar la diversidad se eligen subconjuntos disjuntos. 
	\item Para cada subconjunto se selecciona aleatoriamente una muestra del tamaño del 75\%. Se ejecuta PCA sobre cada una de las muestras.
	\item De las clases se seleccionan las que son distintas, y de estas se selecciona una muestra del 80\%. Se eligen las instancias que contienen estas clases, y sobre estas se hace el PCA.
\end{itemize}

De esta forma, se construyen clasificadores individuales precisos.